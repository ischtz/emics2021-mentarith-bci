@Article{Stoll2013,
  Title                    = {Pupil responses allow communication in locked-in syndrome patients.},
  Author                   = {Stoll, Josef and Chatelle, Camille and Carter, Olivia and Koch, Christof and Laureys, Steven and Einh{\"{a}}user, Wolfgang},
  Journal                  = {Curr Biol},
  Year                     = {2013},
  Month                    = {Aug},
  Number                   = {15},
  Pages                    = {R647--R648},
  Volume                   = {23},
  Abstract                 = {For patients with severe motor disabilities, a robust means of communication is a crucial factor for their well-being [1]. We report here that pupil size measured by a bedside camera can be used to communicate with patients with locked-in syndrome. With the same protocol we demonstrate command-following for a patient in a minimally conscious state, suggesting its potential as a diagnostic tool for patients whose state of consciousness is in question. Importantly, neither training nor individual adjustment of our system's decoding parameters were required for successful decoding of patients' responses.},
  DOI                      = {10.1016/j.cub.2013.06.011},
  File                     = {:Stoll2013.pdf:PDF},
  Keywords                 = {Adult; Aged; Communication; Humans; Male; Middle Aged; Persistent Vegetative State, diagnosis; Photography, instrumentation; Pupil, physiology; Quadriplegia; Young Adult},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Pii                      = {S0960-9822(13)00702-1},
  Pmid                     = {23928079},
  Timestamp                = {2015.11.12},
  URL                      = {http://dx.doi.org/10.1016/j.cub.2013.06.011}
}


@Article{Schneider2009,
  Title                    = {EyeSeeCam: an eye movement-driven head camera for the examination of natural visual exploration.},
  Author                   = {Schneider, Erich and Villgrattner, Thomas and Vockeroth, Johannes and Bartl, Klaus and Kohlbecher, Stefan and Bardins, Stanislavs and Ulbrich, Heinz and Brandt, Thomas},
  Journal                  = {Ann N Y Acad Sci},
  Year                     = {2009},
  Month                    = {May},
  Pages                    = {461--467},
  Volume                   = {1164},
  Abstract                 = {The prototype of a gaze-controlled, head-mounted camera (EyeSeeCam) was developed that provides the functionality for fundamental studies on human gaze behavior even under dynamic conditions like locomotion. EyeSeeCam incorporates active visual exploration by saccades with image stabilization during head, object, and surround motion just as occurs in human ocular motor control. This prototype is a first attempt to combine free user mobility with image stabilization and unrestricted exploration of the visual surround in a man-made technical vision system. The gaze-driven camera is supplemented by an additional wide-angle, head-fixed scene camera. In this scene view, the focused gaze view is embedded with picture-in-picture functionality, which provides an approximation of the foveated retinal content. Such a combined video clip can be viewed more comfortably than the saccade-pervaded image of the gaze camera alone. EyeSeeCam consists of a video-oculography (VOG) device and a camera motion device. The benchmark for the evaluation of such a device is the vestibulo-ocular reflex (VOR), which requires a latency on the order of 10 msec between head and eye (camera) movements for proper image stabilization. A new lightweight VOG was developed that is able to synchronously measure binocular eye positions at up to 600 Hz. The camera motion device consists of a parallel kinematics setup with a backlash-free gimbal joint that is driven by piezo actuators with no reduction gears. As a result, the latency between the rotations of an artificial eye and the camera was 10 msec, which is VOR-like.},
  DOI                      = {10.1111/j.1749-6632.2009.03858.x},
  File                     = {:Schneider2009.pdf:PDF},
  Institution              = {Clinical Neurosciences, University of Munich Hospital, Munich, Germany. eschneider@nefo.med.uni-muenchen.de},
  Keywords                 = {Biomechanical Phenomena; Eye Movements; Humans; Oculomotor Muscles, physiology; Photography, instrumentation; Reaction Time},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Pii                      = {NYAS03858},
  Pmid                     = {19645949},
  Timestamp                = {2015.07.17},
  URL                      = {http://dx.doi.org/10.1111/j.1749-6632.2009.03858.x}
}

@Article{Stoll2011,
  author          = {Stoll, Josef and Kohlbecher, Stefan and Marx, Svenja and Schneider, Erich and Einhäuser, Wolfgang},
  title           = {Mobile three dimensional gaze tracking.},
  journal         = {Studies in health technology and informatics},
  year            = {2011},
  volume          = {163},
  pages           = {616--622},
  issn            = {0926-9630},
  abstract        = {Mobile eyetracking is a recent method enabling research on attention during real-life behavior. With the EyeSeeCam, we have recently presented a mobile eye-tracking device, whose camera-motion device (gazecam) records movies orientated in user's direction of gaze. Here we show that the EyeSeeCam can extract a reliable vergence signal, to measure the fixation distance. We extend the system to determine not only the direction of gaze for short distances more precisely, but also the fixation point in 3 dimensions (3D). Such information is vital, if gaze-tracking shall be combined with tasks requiring 3D information in the peri-personal space, such as grasping. Hence our method substantially extends the application range for mobile gaze-tracking devices and makes a decisive step towards their routine application in standardized clinical settings.},
  citation-subset = {T},
  completed       = {2011-05-17},
  country         = {Netherlands},
  created         = {2011-02-21},
  file            = {:Stoll2011 - Mobile three dimensional gaze tracking..pdf:PDF},
  issn-linking    = {0926-9630},
  keywords        = {Equipment Design; Equipment Failure Analysis; Eye Movements, physiology; Fixation, Ocular, physiology; Humans; Imaging, Three-Dimensional, instrumentation; Monitoring, Ambulatory, instrumentation; Photography, instrumentation; Signal Processing, Computer-Assisted, instrumentation; Video Recording, instrumentation},
  nlm-id          = {9214582},
  owner           = {NLM},
  pmid            = {21335867},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2011-02-21},
}

@article{Mathot2012,
  title={OpenSesame: An open-source, graphical experiment builder for the social sciences},
  author={Math{\^o}t, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
  journal={Behavior research methods},
  volume={44},
  number={2},
  pages={314--324},
  year={2012},
  publisher={Springer}
}

@article{Dalmaijer2014,
  title={PyGaze: An open-source, cross-platform toolbox for minimal-effort programming of eyetracking experiments},
  author={Dalmaijer, Edwin S and Math{\^o}t, Sebastiaan and Van der Stigchel, Stefan},
  journal={Behavior research methods},
  volume={46},
  number={4},
  pages={913--921},
  year={2014},
  publisher={Springer}
}

@Article{Naber2013,
  Title                    = {Tracking the allocation of attention using human pupillary oscillations.},
  Author                   = {Naber, Marnix and Alvarez, George A. and Nakayama, Ken},
  Journal                  = {Front Psychol},
  Year                     = {2013},
  Pages                    = {919},
  Volume                   = {4},
  Abstract                 = {The muscles that control the pupil are richly innervated by the autonomic nervous system. While there are central pathways that drive pupil dilations in relation to arousal, there is no anatomical evidence that cortical centers involved with visual selective attention innervate the pupil. In this study, we show that such connections must exist. Specifically, we demonstrate a novel Pupil Frequency Tagging (PFT) method, where oscillatory changes in stimulus brightness over time are mirrored by pupil constrictions and dilations. We find that the luminance-induced pupil oscillations are enhanced when covert attention is directed to the flicker stimulus and when targets are correctly detected in an attentional tracking task. These results suggest that the amplitudes of pupil responses closely follow the allocation of focal visual attention and the encoding of stimuli. PFT provides a new opportunity to study top-down visual attention itself as well as identifying the pathways and mechanisms that support this unexpected phenomenon.},
  DOI                      = {10.3389/fpsyg.2013.00919},
  File                     = {:NaberAlvarezNakayama2013.pdf:PDF},
  Institution              = {Vision Sciences Laboratory, Department of Psychology, Harvard University Cambridge, MA, USA.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Pmc                      = {PMC3857913},
  Pmid                     = {24368904},
  URL                      = {http://dx.doi.org/10.3389/fpsyg.2013.00919}
}

@Article{Mathot2015,
  Title                    = {The mind-writing pupil: near-perfect decoding of visual attention with pupillometry.},
  Author                   = {Math{\^{o}}t, Sebastiaan and Melmi, Jean-Baptiste and {Van der Linden}, Lotje and {Van der Stigchel}, Stefan},
  Journal                  = {J Vis},
  Year                     = {2015},
  Month                    = {Sep},
  Number                   = {12},
  Pages                    = {176},
  Volume                   = {15},
  Abstract                 = {When the eyes are exposed to light, the pupils constrict. This is the well known pupillary light response. What is less well known is that the pupillary light response is not a simple reflex to light, but is modulated by visual attention: When you covertly attend to a bright stimulus, your pupil constricts relative to when you attend to a dark stimulus. Here we describe a human-computer interface that is based on this principle, i.e. decoding the focus of covert visual attention with pupillometry. Participants fixated in the center of a display, and selected (i.e. covertly attended to) one of several stimuli presented in a circular arrangement. Each stimulus was presented on a background with alternating brightness. Small changes in pupil size reflected the brightness alternations of the selected stimulus' background, and this allowed us to determine which stimulus was selected with nearly perfect accuracy on a trial-by-trial basis. An extension of this technique, in which the stimulus array serves as a virtual keyboard, even allows for arbitrary text input. As a human-computer interface, this technique has several key advantages: It is intuitive, because there is a direct mapping between the task (attending to a stimulus) and the goal (selecting a stimulus); It allows for bi-directional communication, because the analysis can be performed on-line; It is non-invasive and can be done with currently available low-cost eye trackers; It is reliable, i.e. decoding accuracy is nearly perfect under good conditions. We discuss potential applications, such as communication with locked-in patients and ultra-secure password input. Meeting abstract presented at VSS 2015.},
  DOI                      = {10.1167/15.12.176},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Pii                      = {2433214},
  Pmid                     = {26325864},
  Timestamp                = {2015.11.11},
  URL                      = {http://dx.doi.org/10.1167/15.12.176}
}

@Article{Mathot2016,
  author      = {Math{\^{o}}t, Sebastiaan and Melmi, Jean-Baptiste and {van der Linden}, Lotje and {Van der Stigchel}, Stefan},
  title       = {The Mind-Writing Pupil: A Human-Computer Interface Based on Decoding of Covert Attention through Pupillometry.},
  journal     = {PLoS One},
  year        = {2016},
  volume      = {11},
  number      = {2},
  pages       = {e0148805},
  abstract    = {We present a new human-computer interface that is based on decoding of attention through pupillometry. Our method builds on the recent finding that covert visual attention affects the pupillary light response: Your pupil constricts when you covertly (without looking at it) attend to a bright, compared to a dark, stimulus. In our method, participants covertly attend to one of several letters with oscillating brightness. Pupil size reflects the brightness of the selected letter, which allows us-with high accuracy and in real time-to determine which letter the participant intends to select. The performance of our method is comparable to the best covert-attention brain-computer interfaces to date, and has several advantages: no movement other than pupil-size change is required; no physical contact is required (i.e. no electrodes); it is easy to use; and it is reliable. Potential applications include: communication with totally locked-in patients, training of sustained attention, and ultra-secure password input.},
  doi         = {10.1371/journal.pone.0148805},
  file        = {:Mathot2016.pdf:PDF},
  groups      = {DEPTH},
  institution = {Dept. of Experimental Psychology, Helmholtz Institute, Utrecht University, Utrecht, The Netherlands.},
  language    = {eng},
  medline-pst = {epublish},
  pii         = {PONE-D-15-47494},
  pmc         = {PMC4743834},
  pmid        = {26848745},
  url         = {http://dx.doi.org/10.1371/journal.pone.0148805},
}

@phdthesis{Diedrichs2015,
  title={Leveraging Pupillometry and Luminance-Based Mental Imagery for a Novel Mode of Communication},
  author={Diedrichs, Victoria Anne},
  year={2015},
  school={Temple University. Libraries}
}

@article{Laeng2014,
  title={The eye pupil adjusts to imaginary light},
  author={Laeng, Bruno and Sulutvedt, Unni},
  journal={Psychological science},
  volume={25},
  number={1},
  pages={188--197},
  year={2014},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{Ponzio2019,
  title={A human-computer interface based on the “voluntary” pupil accommodative response},
  author={Ponzio, Francesco and Villalobos, Andres Eduardo Lorenzo and Mesin, Luca and de'Sperati, Claudio and Roatta, Silvestro},
  journal={International Journal of Human-Computer Studies},
  volume={126},
  pages={53--63},
  year={2019},
  publisher={Elsevier}
}

@article{Kasthurirangan2005,
  title={Characteristics of pupil responses during far-to-near and near-to-far accommodation},
  author={Kasthurirangan, Sanjeev and Glasser, Adrian},
  journal={Ophthalmic and Physiological Optics},
  volume={25},
  number={4},
  pages={328--339},
  year={2005},
  publisher={Wiley Online Library}
}

@inproceedings{Strauch2017,
  title={Pupil-assisted target selection (pats)},
  author={Strauch, Christoph and Ehlers, Jan and Huckauf, Anke},
  booktitle={Ifip conference on human-computer interaction},
  pages={297--312},
  year={2017},
  organization={Springer}
}
@inproceedings{Strauch2017a,
  title={Towards pupil-assisted target selection in natural settings: Introducing an on-screen keyboard},
  author={Strauch, Christoph and Greiter, Lukas and Huckauf, Anke},
  booktitle={IFIP Conference on Human-Computer Interaction},
  pages={534--543},
  year={2017},
  organization={Springer}
}

@article{Hayes2016,
  title={Mapping and correcting the influence of gaze position on pupil size measurements},
  author={Hayes, Taylor R and Petrov, Alexander A},
  journal={Behavior Research Methods},
  volume={48},
  number={2},
  pages={510--527},
  year={2016},
  publisher={Springer}
}

@inproceedings{Drewes2012,
  title={Shifts in reported gaze position due to changes in pupil size: Ground truth and compensation},
  author={Drewes, Jan and Masson, Guillaume S and Montagnini, Anna},
  booktitle={Proceedings of the symposium on eye tracking research and applications},
  pages={209--212},
  year={2012}
}

@article{Mathot2018,
  title={Safe and sensible preprocessing and baseline correction of pupil-size data},
  author={Math{\^o}t, Sebastiaan and Fabius, Jasper and Van Heusden, Elle and Van der Stigchel, Stefan},
  journal={Behavior research methods},
  volume={50},
  number={1},
  pages={94--106},
  year={2018},
  publisher={Springer}
}

@article{Hess1964,
  title={Pupil size in relation to mental activity during simple problem-solving},
  author={Hess, Eckhard H and Polt, James M},
  journal={Science},
  volume={143},
  number={3611},
  pages={1190--1192},
  year={1964},
  publisher={American Association for the Advancement of Science}
}

@article{Beatty2000,
  title={The pupillary system},
  author={Beatty, Jackson and Lucero-Wagoner, Brennis and others},
  journal={Handbook of psychophysiology},
  volume={2},
  number={142-162},
  year={2000}
}

@article{Robin2011,
  title={pROC: an open-source package for R and S+ to analyze and compare ROC curves},
  author={Robin, Xavier and Turck, Natacha and Hainard, Alexandre and Tiberti, Natalia and Lisacek, Fr{\'e}d{\'e}rique and Sanchez, Jean-Charles and M{\"u}ller, Markus},
  journal={BMC bioinformatics},
  volume={12},
  number={1},
  pages={1--8},
  year={2011},
  publisher={BioMed Central}
}

@inproceedings{Duchowski2020,
  title={The Low/High Index of Pupillary Activity},
  author={Duchowski, Andrew T and Krejtz, Krzysztof and Gehrer, Nina A and Bafna, Tanya and B{\ae}kgaard, Per},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2020}
}

@inproceedings{Duchowski2018,
  title={The index of pupillary activity: Measuring cognitive load vis-{\`a}-vis task difficulty with pupil oscillation},
  author={Duchowski, Andrew T and Krejtz, Krzysztof and Krejtz, Izabela and Biele, Cezary and Niedzielska, Anna and Kiefer, Peter and Raubal, Martin and Giannopoulos, Ioannis},
  booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2018}
}

@article{deLong1988,
  title={Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach},
  author={DeLong, Elizabeth R and DeLong, David M and Clarke-Pearson, Daniel L},
  journal={Biometrics},
  pages={837--845},
  year={1988},
  publisher={JSTOR}
}

@incollection{Einhauser2017,
  title={The pupil as marker of cognitive processes},
  author={Einh{\"a}user, Wolfgang},
  booktitle={Computational and cognitive neuroscience of vision},
  pages={141--169},
  year={2017},
  publisher={Springer}
}

@article{Mathot2018review,
  title={Pupillometry: Psychology, physiology, and function},
  author={Math{\^o}t, Sebastiaan},
  journal={Journal of Cognition},
  volume={1},
  number={1},
  year={2018},
  publisher={Ubiquity Press}
}

@Article{Heinrich1896,
  author  = {Heinrich, W.},
  journal = {Zeitschrift für Psychologie und Physiologie der Sinnesorgane},
  title   = {Die Aufmerksamkeit und die Funktion der Sinnesorgane: Erster Beitrag.},
  year    = {1986},
  pages   = {342-388},
  volume  = {9},
}

@article{Ahern1979,
  title={Pupillary responses during information processing vary with Scholastic Aptitude Test scores},
  author={Ahern, Sylvia and Beatty, Jackson},
  journal={Science},
  volume={205},
  number={4412},
  pages={1289--1292},
  year={1979},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{Klingner2008,
  title={Measuring the task-evoked pupillary response with a remote eye tracker},
  author={Klingner, Jeff and Kumar, Rakshit and Hanrahan, Pat},
  booktitle={Proceedings of the 2008 symposium on Eye tracking research \& applications},
  pages={69--72},
  year={2008}
}

@inproceedings{Marshall2002ICA,
  title={The index of cognitive activity: Measuring cognitive workload},
  author={Marshall, Sandra P},
  booktitle={Proceedings of the IEEE 7th conference on Human Factors and Power Plants},
  pages={7--7},
  year={2002},
  organization={IEEE}
}

@article{Beatty1982,
  title={Task-evoked pupillary responses, processing load, and the structure of processing resources.},
  author={Beatty, Jackson},
  journal={Psychological bulletin},
  volume={91},
  number={2},
  pages={276},
  year={1982},
  publisher={American Psychological Association}
}

@article{Chen2014,
  title={Using task-induced pupil diameter and blink rate to infer cognitive load},
  author={Chen, Siyuan and Epps, Julien},
  journal={Human--Computer Interaction},
  volume={29},
  number={4},
  pages={390--413},
  year={2014},
  publisher={Taylor \& Francis}
}

@article{Kahneman1966,
  title={Pupil diameter and load on memory},
  author={Kahneman, Daniel and Beatty, Jackson},
  journal={Science},
  volume={154},
  number={3756},
  pages={1583--1585},
  year={1966},
  publisher={American Association for the Advancement of Science}
}
@article{Ehlers2016,
  title={Pupil size changes as an active information channel for biofeedback applications},
  author={Ehlers, Jan and Strauch, Christoph and Georgi, Juliane and Huckauf, Anke},
  journal={Applied psychophysiology and biofeedback},
  volume={41},
  number={3},
  pages={331--339},
  year={2016},
  publisher={Springer}
}
@inproceedings{Ehlers2015,
  title={Towards Voluntary Pupil Control-Training Affective Strategies?},
  author={Ehlers, Jan and Bubalo, Nikola and Loose, Markus and Huckauf, Anke},
  booktitle={International Conference on Physiological Computing Systems},
  volume={2},
  pages={5--12},
  year={2015},
  organization={SCITEPRESS}
}

@inproceedings{Ehlers2018training,
  title={Training facilitates cognitive control on pupil dilation},
  author={Ehlers, Jan and Strauch, Christoph and Huckauf, Anke},
  booktitle={Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
  pages={1--5},
  year={2018}
}

@article{Ehlers2018view,
  title={A view to a click: pupil size changes as input command in eyes-only human-computer interaction},
  author={Ehlers, Jan and Strauch, Christoph and Huckauf, Anke},
  journal={International Journal of Human-Computer Studies},
  volume={119},
  pages={28--34},
  year={2018},
  publisher={Elsevier}
}

@book{majaranta2011gaze,
  title={Gaze Interaction and Applications of Eye Tracking: Advances in Assistive Technologies: Advances in Assistive Technologies},
  author={Majaranta, P{\"a}ivi},
  year={2011},
  publisher={IGI Global}
}

@article{Birbaumer1999,
  title={A spelling device for the paralysed},
  author={Birbaumer, Niels and Ghanayim, Nimr and Hinterberger, Thilo and Iversen, Iver and Kotchoubey, Boris and K{\"u}bler, Andrea and Perelmouter, Juri and Taub, Edward and Flor, Herta},
  journal={Nature},
  volume={398},
  number={6725},
  pages={297--298},
  year={1999},
  publisher={Nature Publishing Group}
}
@article{borgheai2020enhancing,
  title={Enhancing communication for people in late-stage ALS using an fNIRS-based BCI system},
  author={Borgheai, Seyyed Bahram and McLinden, John and Zisk, Alyssa Hillary and Hosni, Sarah Ismail and Deligani, Roohollah Jafari and Abtahi, Mohammadreza and Mankodiya, Kunal and Shahriari, Yalda},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume={28},
  number={5},
  pages={1198--1207},
  year={2020},
  publisher={IEEE}
}

@article{wangwiwattana2018pupilnet,
  title={Pupilnet, measuring task evoked pupillary response using commodity rgb tablet cameras: Comparison to mobile, infrared gaze trackers for inferring cognitive load},
  author={Wangwiwattana, Chatchai and Ding, Xinyi and Larson, Eric C},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={1},
  number={4},
  pages={1--26},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{mazziotti2021meye,
  title={MEYE: Web-app for translational and real-time pupillometry},
  author={Mazziotti, Raffaele M and Carrara, Fabio and Viglione, Aurelia and Lupori, Leonardo and Verde, Luca Lo and Benedetto, Alessandro and Ricci, Giulia and Sagona, Giulia and Amato, Giuseppe and Pizzorusso, Tommaso},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}